# Deploy LLM on sagemaker

使用 LMI 容器在 Sagemker 部署模型做推理

## 相关文档

DLC上的LMI容器列表，请点击[这里](https://github.com/aws/deep-learning-containers/blob/master/available_images.md#large-model-inference-containers)。

对于可用的BYOC容器列表, 请点击[这里](https://hub.docker.com/r/deepjavalibrary/djl-serving/tags)。

有关SageMaker上的LMI文档的更多信息,请点击[这里](https://docs.aws.amazon.com/sagemaker/latest/dg/realtime-endpoints-large-model-inference.html)。

对于您可以在DJLServing上设置的所有serving.properties选项,请点击[这里](https://docs.djl.ai/docs/serving/serving/docs/modes.html#servingproperties)。
