{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c518e46-1464-4cc9-83ca-d992364ae438",
   "metadata": {},
   "source": [
    "# AOT 方式使用 Tensorrt-LMI 部署Baichuan2-13B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5ed753-95bb-411e-a132-38f4002531f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install sagemaker --upgrade  --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925d01e1-1153-4005-9dcf-5cca9a3f3f2d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import Model, image_uris, serializers, deserializers\n",
    "\n",
    "role = sagemaker.get_execution_role()  # execution role for the endpoint\n",
    "sess = sagemaker.session.Session()  # sagemaker session for interacting with different AWS APIs\n",
    "region = sess._region_name  # region name of the current SageMaker Studio environment\n",
    "account_id = sess.account_id()  # account_id of the current SageMaker Studio environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa2eb33-d9aa-4249-9063-05903414f5c7",
   "metadata": {},
   "source": [
    "## Pull 转换模型时需要的镜像"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75da62b6-454f-4a51-ae0e-c26d7b970435",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!aws ecr-public get-login-password --region us-east-1 | docker login --username AWS --password-stdin public.ecr.aws/s0w3f1p2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5837f0c-df31-4a04-90a7-c0c10d4e57e0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!docker pull public.ecr.aws/s0w3f1p2/tensorrt-lmi-xq:v1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba2e611-604f-45f4-9547-0272eae2e939",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 转换模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbbb0977-5c9d-42be-b1c0-b08a3dcbbe25",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile serving.properties\n",
    "option.model_id=baichuan-inc/Baichuan2-13B-Chat\n",
    "option.tensor_parallel_degree=2\n",
    "option.max_rolling_batch_size=64\n",
    "option.dtype=fp16\n",
    "option.baichuan_model_version=v2_13b\n",
    "option.trust_remote_code=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c72d7a-c8a0-4dd7-83f6-569f5cd353eb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!cat serving.properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49b3e4b-b5af-4035-9acc-68817f5f47bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "MODEL_REPO_DIR=\"./output/baichuan_v2_13B_aot2\"\n",
    "!mkdir -p $MODEL_REPO_DIR\n",
    "!mv serving.properties $MODEL_REPO_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ecf8687-64a1-46e0-8605-dc2488f3a45b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bucket=\"llm-trt\"\n",
    "s3_model_prefix = \"lmi/baichuan_v2_13B_aot_2p_2_64\"\n",
    "s3url=f\"s3://{bucket}/{s3_model_prefix}\"\n",
    "s3url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904fc642-8cf0-475d-85d3-ba201aee13e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! readlink -f $MODEL_REPO_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5c2061-96ac-4f8f-b252-d5935c2f9b4f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!docker run --runtime=nvidia --gpus all --shm-size 12gb \\\n",
    "-v /home/ec2-user/SageMaker/output/baichuan_v2_13B_aot2:/tmp/trtllm \\\n",
    "-p 8080:8080 \\\n",
    "public.ecr.aws/s0w3f1p2/tensorrt-lmi-xq:v1 /opt/djl/partition/trt_llm_partition.py \\\n",
    "--properties_dir /tmp/trtllm \\\n",
    "--trt_llm_model_repo /tmp/trtllm \\\n",
    "--tensor_parallel_degree 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cadddcce-81fa-42a7-8d4e-e05afa7d14c4",
   "metadata": {},
   "source": [
    "## 上传转换后的模型到 S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a616c75f-bc72-4490-839d-98c0e35f1fd7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!aws s3 sync /home/ec2-user/SageMaker/output/baichuan_v2_13B_aot2 $s3url"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8de9e3b-5f6b-4122-9112-56147b6e764a",
   "metadata": {},
   "source": [
    "## 创建，上传，部署所需配置文件到S3\n",
    " - 修改 serving.properties 中 model_id 为上传的 S3 模型地址\n",
    " - 根据自己的输入输出，写 model.py 文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca46be6a-52b4-44ff-aab8-cf736d7de4a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile serving.properties\n",
    "option.model_id=s3://llm-trt/lmi/baichuan_v2_13B_aot_2p_2_64\n",
    "option.tensor_parallel_degree=2\n",
    "option.max_rolling_batch_size=64\n",
    "option.dtype=fp16\n",
    "option.rolling_batch=trtllm\n",
    "option.baichuan_model_version=v2_13b\n",
    "option.trust_remote_code=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e54b766-54f2-42f4-9a47-08a0ce589e9e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile model.py\n",
    "from djl_python.tensorrt_llm import TRTLLMService\n",
    "from djl_python.inputs import Input\n",
    "from djl_python.outputs import Output\n",
    "from djl_python.encode_decode import encode, decode\n",
    "import logging\n",
    "import json\n",
    "import types\n",
    "import re\n",
    "\n",
    "_service = TRTLLMService()\n",
    "\n",
    "def custom_output_formatter(token, first_token, last_token, details, generated_tokens):\n",
    "    \"\"\"\n",
    "    Replace this function with your custom output formatter.\n",
    "    \n",
    "    Args:\n",
    "        token (Token): Token object \n",
    "        first (bool): If first token \n",
    "        last (bool): If last token\n",
    "        aux (dict): Miscellaneous information\n",
    "        prev_response (str): Previously generated tokens\n",
    "\n",
    "    Returns:\n",
    "        (str): Response string\n",
    "        \n",
    "    \"\"\"\n",
    "    result = {\"token_id\": token.id, \"token_text\": token.text, \"token_log_prob\": token.log_prob, \"token_special_token\": token.special_token}\n",
    "    final_response = \"\" \n",
    "    if last_token:\n",
    "        # result[\"generated_text\"] = generated_tokens\n",
    "        final_response = re.sub(\"。</s>$\", \"\", generated_tokens) \n",
    "        if details:\n",
    "            result[\"details\"] = {\n",
    "                \"finish_reason\": details.get(\"finish_reason\", None)\n",
    "            }\n",
    "    \n",
    "    #return json.dumps(result, ensure_ascii=False) + \"\\n\"\n",
    "    return final_response\n",
    "\n",
    "def custom_input_formatter(self, inputs):\n",
    "    \"\"\"\n",
    "    Replace this function with your custom input formatter.\n",
    "        \n",
    "    Args:\n",
    "        data (obj): The request data, dict or string  \n",
    "\n",
    "    Returns:\n",
    "        (tuple): input_data (list), input_size (list), parameters (dict), errors (dict), batch (list)\n",
    "    \"\"\"\n",
    "    input_data = []\n",
    "    input_size = []\n",
    "    parameters = []\n",
    "    errors = {}\n",
    "    batch = inputs.get_batches()\n",
    "    for i, item in enumerate(batch):\n",
    "        try:\n",
    "            content_type = item.get_property(\"Content-Type\")\n",
    "            input_map = decode(item, content_type)\n",
    "        except Exception as e:  # pylint: disable=broad-except\n",
    "            logging.warning(f\"Parse input failed: {i}\")\n",
    "            input_size.append(0)\n",
    "            errors[i] = str(e)\n",
    "            continue\n",
    "\n",
    "        _inputs = input_map.pop(\"inputs\", input_map)\n",
    "        print(f\"Dongxq self model.py _inputs: {_inputs}\")\n",
    "        if not isinstance(_inputs, list):\n",
    "            _inputs = [_inputs]\n",
    "        _inputs=list(map(lambda x: f\"<reserved_106>{x}<reserved_107>\", _inputs))\n",
    "        input_data.extend(_inputs)\n",
    "        input_size.append(len(_inputs))\n",
    "\n",
    "        _param = input_map.pop(\"parameters\", {})\n",
    "        if \"cached_prompt\" in input_map:\n",
    "            _param[\"cached_prompt\"] = input_map.pop(\"cached_prompt\")\n",
    "        if not \"seed\" in _param:\n",
    "            # set server provided seed if seed is not part of request\n",
    "            if item.contains_key(\"seed\"):\n",
    "                _param[\"seed\"] = item.get_as_string(key=\"seed\")\n",
    "        for _ in range(input_size[i]):\n",
    "            parameters.append(_param)\n",
    "\n",
    "    return input_data, input_size, parameters, errors, batch\n",
    "\n",
    "def handle(inputs: Input):\n",
    "    \"\"\"\n",
    "    Default handler function\n",
    "    \"\"\"\n",
    "    if not _service.initialized:\n",
    "        # stateful model\n",
    "        props = inputs.get_properties()\n",
    "        print(f\"props: {props}\")\n",
    "        props['output_formatter'] = custom_output_formatter\n",
    "        _service.initialize(props)\n",
    "        _service.parse_input = types.MethodType(custom_input_formatter, _service)\n",
    "\n",
    "    if inputs.is_empty():\n",
    "        # initialization request\n",
    "        return None\n",
    "\n",
    "    return _service.inference(inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6d70a2-b5dc-4b98-9627-6993c1dba267",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "mkdir mymodel\n",
    "mv serving.properties model.py mymodel/\n",
    "tar czvf mymodel.tar.gz mymodel/\n",
    "rm -rf mymodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744579de-f8fa-450a-89a1-329e84e067bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_code_prefix = \"large-model-lmi/baichuan-v2-13B-aot-code\"\n",
    "bucket = sess.default_bucket()  # bucket to house artifacts\n",
    "code_artifact = sess.upload_data(\"mymodel.tar.gz\", bucket, s3_code_prefix)\n",
    "print(f\"S3 Code or Model tar ball uploaded to --- > {code_artifact}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a15ffed-5c5b-4809-b4de-512aad3291a4",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 设置部署使用的镜像初始化model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df140c25-c20c-4dd0-b306-286fc57ce52b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "image_uri = image_uris.retrieve(\n",
    "        framework=\"djl-tensorrtllm\",\n",
    "        region=sess.boto_session.region_name,\n",
    "        version=\"0.26.0\"\n",
    "    )\n",
    "\n",
    "model = Model(image_uri=image_uri, model_data=code_artifact, role=role)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "411ac2af-54c7-44c2-8ee7-3239526570aa",
   "metadata": {},
   "source": [
    "## 开始部署"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56e2d51-2800-4524-a484-1d7cacdb2bf6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "instance_type = \"ml.g5.12xlarge\"\n",
    "endpoint_name = sagemaker.utils.name_from_base(\"baichuan-v2-13B-aot-2p-64\")\n",
    "\n",
    "model.deploy(initial_instance_count=1,\n",
    "             instance_type=instance_type,\n",
    "             endpoint_name=endpoint_name,\n",
    "             # container_startup_health_check_timeout=3600\n",
    "            )\n",
    "\n",
    "# our requests and responses will be in json format so we specify the serializer and the deserializer\n",
    "predictor = sagemaker.Predictor(\n",
    "    endpoint_name=endpoint_name,\n",
    "    sagemaker_session=sess,\n",
    "    serializer=serializers.JSONSerializer()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af75b82-8ee3-4330-b85e-e3b5361ae142",
   "metadata": {},
   "source": [
    "## 测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e86063-b6d3-42c2-9da9-f43d4a6b5ae2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "response = predictor.predict(\n",
    "    {\"inputs\": \"世界上第二高的山峰是哪座\", \"parameters\": {\"max_new_tokens\":128, \"top_k\":5, \"repetition_penalty\": 1.05, \"top_p\": 0.85, \"pad_id\":0,\"temperature\":0.3}}\n",
    ")\n",
    "\n",
    "text = str(response, 'utf-8')\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99776b0-e8be-4b49-82d9-f35726925efc",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 删除部署的endpoint以及对应的cogfug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f70111-21c4-42c7-835c-259a7b7918ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sess.delete_endpoint(endpoint_name)\n",
    "sess.delete_endpoint_config(endpoint_name)\n",
    "model.delete_model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
